# Project testting document

## Unit Test Coverage Report

Unit test coverage report is generated with coverage.py and is visible [here](https://viljamilatvala.github.io/2048-algorithms/documentation/coverage_report/index.html)

## Unit Testing Implementation

Unit tests are created with the standard library's unittest framework. Currently tests exist for the GameHandler -class and the MiniMaxPlayer -class. Not included in unit testing are User Interface in /src/app.py, browser controls in BrowserGame -class and UserInputPlayer -class which itself is used in manually testing BrowserGame and the UI.

Unit tests can be run from the project root by calling

```
>coverage run -m unittest discover
```

After running the test, a HTML report can be generated by caclling

```
>coverage html
```

## Manual Testing

UserInputPlayer -class was created to prompt and relay move commands from the user. This way we can verify that the game loop and browser controls work as expected.

This test can be replicated by running /src/app.py, selecting UserInputPlayer as the player, and entering the wished amount of games to play. After that the user will be prompted for next moves, and they can inspect if the game obeys those instructions, loops and opens/closes new games correctly.

## Performance Testing

### Algorithm Speed

For the algorithm to be sensible, it should be able to make a move in relatively short period of time. As a rule of thumb I'd say 1 second per move. At its current state the algorithm is capable of adhering to that constraint with search depth of 6. This of course varies depending on computer used. By playing three games on depths 4,5 & 6 we get following average times per move:

| Depth | AVG time per move (sec) |
| ----- | ----------------------- |
| 4     | 0.039                   |
| 5     | 0.201                   |
| 6     | 0.684                   |

Worst case time complexity of minimax should be O(s^d) where s is the number of possible moves from each state and d is the depth of the search. For 2048 the exact quantity of possible moves varies based on the state of the grid, and there are optimizations in the application to reduce the number of moves if they are identical in terms of effect to the next states. The number of possible moves decreases the fuller the game grid gets, and when there are only few tiles left the algorithm is more likely to reach end states. The number of round played (player moves) for different depths were the following:

| Depth | AVG moves per game |
| ----- | ------------------ |
| 4     | 507.3              |
| 5     | 538.7              |
| 6     | 639                |

The longer the game is the more moves are made on fuller grids. For this reason the actual consumed time does not rise as steeply as the theoretical time complexity

### Game Performance

With the limited conducted play testing it would seem that the higher the search depth, the longer the AI survives in the game. Scorewise it is hard to compare the performance due to the low count of recorded games in thus far. In the played 3 games with depths 4,5 & 6 each the highest achieved tiles have been the following

| Depth | 256 | 512 | 1024 |
| ----- | --- | --- | ---- |
| 4     | 2   | 0   | 1    |
| 5     |     | 2   | 1    |
| 6     | 0   | 2   | 1    |

So far the algorithm is not reaching very high scores. In not recorded tests the score of 2048 was reached with depths 5 and 6, but more testing is needed. In addition to increasing the search depth, also the heuristic function should be adjusted.
